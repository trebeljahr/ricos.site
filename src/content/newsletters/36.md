---
title: "Open AI’s Sora, AI Sound Effects, and Gemini 1.5"
cover:
    src: "/assets/midjourney/robot-filming-a-movie.webp"
    alt: "a robot filming a movie"
    width: 780
    height: 780
excerpt: "Welcome to this edition of Live and Learn. This time with OpenAI and Elevenlabs changing the world of movie creation, how Google is opening access to some of their LLM models, and the Gemini 1.5 announcement. As always, I hope you enjoy this edition of Live and Learn. "
tags: ["video generation", "generative AI", "OpenAI", "text-to-video", "DeepMind"]
---

## ✨ Quote ✨

> The programming language of the future is called human. 

– Jensen Huang - [(source)](https://www.youtube.com/watch?v=ytZcvwZxkrg)

## 🖇️ Links 🖇️

[**Sora** by OpenAI](https://openai.com/sora). Finally, OpenAI has released its take on text-to-video generation. And it seems to be the best out there, a new milestone, much better than anything before, like RunwayML or Pika or Stable Video. The main thing that distinguishes OpenAI’s efforts is that their model can generate videos up to a minute long while keeping the context “alive”. Not like Runways 8 seconds. OpenAI's model might also be using an entirely different and novel architecture, something much closer to a [world model or simulation engine](https://openai.com/research/video-generation-models-as-world-simulators). If video generation models develop with the same speed that generative images did, I think that by the end of this year, we will have photorealistic minute-long videos that can be entirely generated on home laptop devices within mere seconds... I am excited but also terrified of this future. Entirely AI-generated movies will become a thing and we “just” have to do the high-level direction. Cutting production times to a mere fraction of what they are now. I imagine a world where–if you have an idea for creating a movie–you don't go out and cast actors and think about what camera to shoot on etc. but instead use an AI to create the entire thing.

[**AI Sound Effects are coming soon** by ElevenLabs](https://elevenlabs.io/blog/ai-sound-effects-are-coming-soon/). 
The Sora video generation model capabilities can be paired with technology from the likes of ElevenLabs to generate the music and sound effects on top of all of this. What this [looks and feels like *right now*](https://www.youtube.com/watch?v=VDaZ9gTx7A8) is shown in their demo. This is only a small glimpse of what's to come. Maybe by the end of this year, AI will not just handle the visual aspects of Hollywood… To me, the most crazy thing about all of this is how empowering this technology feels like. It means that *everybody* can generate amazing videos without needing a big budget for a RED camera or advanced CGI or VFX or anything like that. Simply let your creativity run wild and *create* the things you envision with the help of AI. 

[**Stable Diffusion 3** by StabilityAI](https://stability.ai/news/stable-diffusion-3). StabilityAI has put out the announcement for their new Stable Diffusion models. They are still in the beta testing phase right now but will be rolled out and openly available soon. The most notable thing about it is that the model is much better at adding text into generated images *accurately*. Stable Diffusion also recently released [Stable Cascade](https://stability.ai/news/introducing-stable-cascade), another image generation model that can run on less demanding hardware, that is built on an entirely different architecture.

[**Gemma Open Source Models** by Deepmind](https://blog.google/technology/developers/gemma-open-models/). Google makes part of its LLM efforts more accessible for others to use. They provided multiple open-source models they call “Gemma” which are based on the architecture of their big Gemini model. It makes me happy to see a big company like Google hop more onto the open-source train, especially within the world of machine learning. 

[**Gemini 1.5** by Google](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/). Google is opening access to its Gemini Ultra models. To me the craziest aspect is the length of their context windows: In their own words: 
> "This new generation also delivers a breakthrough in long-context understanding. We’ve been able to significantly increase the amount of information our models can process — running up to 1 million tokens consistently, achieving the longest context window of any large-scale foundation model yet." 
1 million tokens?! This means you can feed it an entire series of books like Harry Potter and it will 
*understand* the whole thing. You can ask it for summaries and quotes and advice and emulating personas etc. from the book. This means soon you'll have something akin to a "write a sequel to this novel" button.

## 🌌 Traveling 🌌

I spent the last two weeks on a boat, sailing across the Atlantic. It was one of the most fun but also challenging experiences of my life and I have more pictures of sunrises and sunsets than I could ever use for anything. Seeing land again after 18 days out at sea felt incredible and the water here in the Carribean is simply beautiful.

![breakfast](/assets/newsletter/transat/breakfast.webp) 
![carribean sunset](/assets/newsletter/transat/carribean-sunset.webp) 
![sunrise heaven](/assets/newsletter/transat/sunrise-heaven.webp) 
![stormclouds](/assets/newsletter/transat/stormclouds.webp) 
![fish](/assets/newsletter/transat/fish.webp) 
![fresh sushi](/assets/newsletter/transat/fresh-sushi.webp) 
![dolphins](/assets/newsletter/transat/dolphins.webp) 
![full sails](/assets/newsletter/transat/full-sails.webp) 
![water gold](/assets/newsletter/transat/water-gold.webp) 
![broken wings](/assets/newsletter/transat/broken-wings.webp) 
![fixing sails](/assets/newsletter/transat/fixing-sails.webp) 
![going for a swim](/assets/newsletter/transat/going-for-a-swim.webp) 
![making dinner](/assets/newsletter/transat/making-dinner.webp) 
![ship](/assets/newsletter/transat/ship.webp) 
![speedy sails](/assets/newsletter/transat/speedy-sails.webp) 
![rainbow](/assets/newsletter/transat/rainbow.webp) 
![fish curry](/assets/newsletter/transat/fish-curry.webp) 
![sunset magic](/assets/newsletter/transat/sunset-magic.webp) 
![weathering the storm](/assets/newsletter/transat/weathering-the-storm.webp)
![carribean](/assets/newsletter/transat/carribean.webp) 

## 🎶 Song 🎶

**Little Blue (Mahogany Session)** by Jacob Collier 

[Youtube Music](https://music.youtube.com/watch?v=1x3grej81Mk) | [Spotify](https://open.spotify.com/track/4dkxQNsQUboUQ2I4wXkE0d)

---

That's all for this time. I hope you found this newsletter useful, beautiful, or even both!

Have ideas for improving it? As always [please let me know](https://airtable.com/shro1VeyG4lkNXkx2). 

Cheers,

**– Rico**
