---
title: "Open AIs Sora, AI Sound Effects, and Gemini 1.5"
cover:
    src: "/assets/midjourney/robot-filming-a-movie.webp"
    alt: "a robot filming a movie"
    width: 780
    height: 780
excerpt: "Welcome to this edition of Live and Learn. This time with the first steps into the future of how movies will be created almost entirely by AI, how Google is opening access to some of their LLM models and the Gemini 1.5 announcement. As always, I hope you enjoy this edition of Live and Learn. "
tags: []
---

## ✨ Quote ✨

> The programming language of the future is called human. 

– Jensen Huang - [(source)](https://www.youtube.com/watch?v=ytZcvwZxkrg)

## 🖇️ Links 🖇️

[**Sora** by OpenAI](https://openai.com/sora). Finally, OpenAI has released their take on text to video generation. And it seems to be the best out there, a new milestone in this technology, much better than the current versions of RunwayML or Pika or Stable Video. The main thing distinguishing open AIs efforts is that their model can generate videos up to a minute long while keeping the context. Not like Runways 8 seconds. OpenAI's model  might also be using a different architecture, something much more closer to a [world model or simulation engine](https://openai.com/research/video-generation-models-as-world-simulators). If video generation models develop with the same speed that generative images did, I think that by the end of this year we will have photorealistic minute long videos that can be entirely generated on home laptop devices within mere seconds... I am excited and somewhat terrified in the face of this future. Unlimited video that is driven by text which in turn is also AI generated will be a real "workflow". Basically AI generated movies will become a thing and humans just have to do high level direction to create engaging content in a mere fraction of the time it used to take. I imagine a world where if you have an idea for creating a movie, you don't go out and cast actors and think about what camera to shoot on, but instead use a model to create the entire visual aspects and storyline with the help of AI. 

[**AI Sound Effects are coming soon** by ElevenLabs](https://elevenlabs.io/blog/ai-sound-effects-are-coming-soon/). 
The Sora video generation models capabilities can be paired with technology from the likes of ElevenLabs to generate the music and sound effects on top of all of this. What this [looks and feels like now](https://www.youtube.com/watch?v=VDaZ9gTx7A8) is shown in their demo and only a small glimpse of the things to come. Maybe within the end of this year AI will not just handle the visual aspects of Hollywood. To me the most crazy thing about all of this is how empowering this technology feels like. It means that *everybody* can generate amazing videos without needing a big budget for shooting or advanced CGI or VFX or anything like that. Simply let your creativity run wild and *create* the things you envision. 

[**Stable Diffusion 3** by StabilityAI](https://stability.ai/news/stable-diffusion-3). Stability AI has put out the announcement for their new Stable Diffusion models. They are still in beta testing phase right now but will be rolled out and openly available soon. The most notable thing about it is that the model is much better at adding text into generated images *accurately*. Stable Diffusion also recently released [Stable Cascade](https://stability.ai/news/introducing-stable-cascade), another image generation model that can run on less demanding hardware, that is built on an entirely different architecture.

[**Gemma Open Source Models** by Deepmind](https://blog.google/technology/developers/gemma-open-models/). Google makes part of their LLM efforts more accessible for others to use. The provided multiple open source models called Gemma which are based on the architecture of their big Gemini models.

[**Gemini 1.5** by Google](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/). Google is opening access to it's Gemini Ultra models. To me the craziest aspect is the length of their context windows: In their own words: 
> "This new generation also delivers a breakthrough in long-context understanding. We’ve been able to significantly increase the amount of information our models can process — running up to 1 million tokens consistently, achieving the longest context window of any large-scale foundation model yet." 
1 million tokens?! This means you can feed it entire series of books like Harry Potter and ask it for summaries and quotes and advice and emulating personas etc. from the book. Which means soon you'll have something akin to a "write the sequel to this novel" button.

## 🌌 Traveling 🌌

The last two weeks I spent on a boat sailing across the Atlantic. It was one of the most fun but also challenging experiences of my life and I have more pictures of sunrises and sunsets that I could ever use for anything. Seeing land again after 18 days out at sea felt incredible and the water here in the Carribean is beautiful. 

![breakfast](/assets/newsletter/transat/breakfast.webp) 
![carribean sunset](/assets/newsletter/transat/carribean-sunset.webp) 
![sunrise heaven](/assets/newsletter/transat/sunrise-heaven.webp) 
![stormclouds](/assets/newsletter/transat/stormclouds.webp) 
![fish](/assets/newsletter/transat/fish.webp) 
![fresh sushi](/assets/newsletter/transat/fresh-sushi.webp) 
![dolphins](/assets/newsletter/transat/dolphins.webp) 
![full sails](/assets/newsletter/transat/full-sails.webp) 
![water gold](/assets/newsletter/transat/water-gold.webp) 
![broken wings](/assets/newsletter/transat/broken-wings.webp) 
![fixing sails](/assets/newsletter/transat/fixing-sails.webp) 
![going for a swim](/assets/newsletter/transat/going-for-a-swim.webp) 
![making dinner](/assets/newsletter/transat/making-dinner.webp) 
![ship](/assets/newsletter/transat/ship.webp) 
![speedy sails](/assets/newsletter/transat/speedy-sails.webp) 
![rainbow](/assets/newsletter/transat/rainbow.webp) 
![fish curry](/assets/newsletter/transat/fish-curry.webp) 
![sunset magic](/assets/newsletter/transat/sunset-magic.webp) 
![weathering the storm](/assets/newsletter/transat/weathering-the-storm.webp)
![carribean](/assets/newsletter/transat/carribean.webp) 



## 🎶 Song 🎶

**Song** by Artist 

[Youtube Music]() | [Spotify]()

---

That's all for this time. I hope you found this newsletter useful, beautiful, or even both!

Have ideas for improving it? As always [please let me know](https://airtable.com/shro1VeyG4lkNXkx2). 

Cheers,

**– Rico**
